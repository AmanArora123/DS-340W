---
title: "R Notebook"
output:
  pdf_document: default
  html_notebook: default
---
```{r}
options(warn=-1)
```

```{r}
#system("sudo apt-get -y install libmagick++-dev", intern=TRUE)
#install.packages("magick", verbose=TRUE)
#install.packages("RWeka")

# Libraries
library(readr)        # reads in CSV
library(ggplot2)      # plot library
library(tidyverse)    # for data manipulation
library(gridExtra)    # multiple plots in 1
library(magick)       # attach dope image for visual
library(scales)       # show the colors
library(ggrepel)      # for graph repel (labels)
library(repr)         # resize graphs
library(hexbin)       # for hive scatter
library(naniar)       # to check for missing data
library(lubridate)    # for date and time
library(tm)
library(wordcloud)    # beautiful wordclouds
library(wordcloud2)
library(tidytext)     # text preprocessing
library(textdata)     # text preprocessing
library(reshape2)
library(knitr)
library(grid)
library(igraph)
library(ggraph)
library(ggsci)
library(devtools)
library(circlize)
library(radarchart)
library(stringr)
library(sjmisc)
library(magick)
library(htmlwidgets)
library(VIM)          # missing values visual
library(colorspace)   # maybe for wordcloud
library(RWeka)
library(textmineR)
```

```{r}
options(repr.plot.width=15, repr.plot.height=7)

# Custom Color Palette
my_colors <- c("#05A4C0", "#85CEDA", "#D2A7D8", "#A67BC5", "#BB1C8B", "#8D266E")
show_col(my_colors, labels = F, borders = NA)


# Custom Theme Variable
my_theme <- theme(plot.background = element_rect(fill = "grey98", color = "grey20"),
                  panel.background = element_rect(fill = "grey98"),
                  panel.grid.major = element_line(colour = "grey87"),
                  text = element_text(color = "grey20"),
                  plot.title = element_text(size = 22),
                  plot.subtitle = element_text(size = 17),
                  axis.title = element_text(size = 15),
                  axis.text = element_text(size = 15),
                  legend.box.background = element_rect(color = "grey20", fill = "grey98", size = 0.1),
                  legend.box.margin = margin(t = 3, r = 3, b = 3, l = 3),
                  legend.title = element_blank(),
                  legend.text = element_text(size = 15),
                  strip.text = element_text(size=17))
```


```{r}
# Import data information
data <- read_csv("~/desktop/Code Implementation/covid-19-all.csv", 
                 col_types = cols('Country/Region '= col_character(),
                                  'Province/State' = col_character(),
                                  Latitude = col_double(),
                                  Longitude = col_double(),
                                 Recovered = col_double(),
                                 Deaths = col_double(),
                                 Date = col_date(format = "")))
data <- data %>% rename(c("Country" = "Country/Region", "State" = "Province/State"))

tweets <- read_csv("~/desktop/Code Implementation//covid19_tweets.csv",
                   col_types = cols(user_name = col_character(),
                                    user_location = col_character(),
                                    user_description = col_character(),
                                    user_created = col_datetime(format = ""),
                                    user_followers = col_double(),
                                    user_friends = col_double(),
                                    user_favourites = col_double(),
                                    user_verified = col_logical(),
                                    date = col_datetime(format = ""),
                                    text = col_character(),
                                    hashtags = col_character(),
                                    source = col_character(),
                                    is_retweet = col_logical()))

# Inspect data
#data %>% head(5)

# Inspect tweet data
#tweets %>% head(2)
```

```{r}
cleanCorpus <- function(text){
  # punctuation, whitespace, lowercase, numbers
  text.tmp <- tm_map(text, removePunctuation)
  text.tmp <- tm_map(text.tmp, stripWhitespace)
  text.tmp <- tm_map(text.tmp, content_transformer(tolower))
  text.tmp <- tm_map(text.tmp, removeNumbers)
  
  # removes stopwords
  stopwords_remove <- c(stopwords("en"), c("thats","weve","hes","theres","ive","im",
                                                "will","can","cant","dont","youve","us",
                                                "youre","youll","theyre","whats","didnt"))
  text.tmp <- tm_map(text.tmp, removeWords, stopwords_remove)

  return(text.tmp)
}
```

```{r}
# --- UNIGRAM ---
frequentTerms <- function(text){
  
  # create the matrix
  s.cor <- VCorpus(VectorSource(text))
  s.cor.cl <- cleanCorpus(s.cor)
  s.tdm <- TermDocumentMatrix(s.cor.cl)
  s.tdm <- removeSparseTerms(s.tdm, 0.999)
  m <- as.matrix(s.tdm)
  word_freqs <- sort(rowSums(m), decreasing = T)
  
  # change to dataframe
  dm <- data.frame(word=names(word_freqs), freq=word_freqs)
  
  return(dm)
}

# --- BIGRAM ---
# Bigram tokenizer
tokenizer_2 <- function(x){
  NGramTokenizer(x, Weka_control(min=2, max=2))
}

# Bigram function 
frequentBigrams <- function(text){

  s.cor <- VCorpus(VectorSource(text))
  s.cor.cl <- cleanCorpus(s.cor)
  s.tdm <- TermDocumentMatrix(s.cor.cl, control=list(tokenize=tokenizer_2))
  s.tdm <- removeSparseTerms(s.tdm, 0.999)
  m <- as.matrix(s.tdm)
  word_freqs <- sort(rowSums(m), decreasing=T)
  dm <- data.frame(word=names(word_freqs), freq=word_freqs)
  
  return(dm)
}

# --- TRIGRAM ---
# Trigram tokenizer
tokenizer_3 <- function(x){
  NGramTokenizer(x, Weka_control(min=3, max=3))
}

# Trigram function 
frequentTrigrams <- function(text){

  s.cor <- VCorpus(VectorSource(text))
  s.cor.cl <- cleanCorpus(s.cor)
  s.tdm <- TermDocumentMatrix(s.cor.cl, control=list(tokenize=tokenizer_3))
  s.tdm <- removeSparseTerms(s.tdm, 0.999)
  m <- as.matrix(s.tdm)
  word_freqs <- sort(rowSums(m), decreasing=T)
  dm <- data.frame(word=names(word_freqs), freq=word_freqs)
  
  return(dm)
}
```

```{r}
afinn <- read_csv("~/Desktop/Code Implementation//Afinn.csv",
                  col_types = cols(word = col_character(), value = col_double()))
bing <- read_csv("~/Desktop/Code Implementation//Bing.csv",
                 col_types = cols(word = col_character(), sentiment = col_character()))
nrc <- read_csv("~/Desktop/Code Implementation//NRC.csv",
                col_types = cols(word = col_character(), sentiment = col_character()))
```

```{r}
# Extract the location to map it separately
tweets_location <- tweets %>%
                        # convert to lower case
                        mutate(user_location = tolower(user_location)) %>%
                        group_by(user_location) %>%
                        summarise(n = n(), .groups = "drop_last") %>%
                        arrange(desc(n))

# Create a new column and fill it with NA
tweets_location$country <- NA
```

```{r}
# Manually map some of the locations - if you have better suggestions, shoot! 
# Cuz I'm desperate for a smarter approach
tweets_location <- tweets_location %>%
                        mutate(country = ifelse(grepl("india", user_location), "India", country),
                               country = ifelse(grepl("delhi", user_location), "India", country),
                               country = ifelse(grepl("mumbai", user_location), "India", country),
                               country = ifelse(grepl("bengaluru", user_location), "India", country),
                               country = ifelse(grepl("bangalore", user_location), "India", country),
                               country = ifelse(grepl("bhubaneswar", user_location), "India", country),
                               country = ifelse(grepl("hyderabad", user_location), "India", country),
                               country = ifelse(grepl("china", user_location), "China", country),
                               country = ifelse(grepl("beijing", user_location), "China", country),
                               country = ifelse(grepl("hong kong", user_location), "Hong Kong", country),
                               country = ifelse(grepl("singapore", user_location), "Singapore", country),
                               country = ifelse(grepl("australia", user_location), "Australia", country),
                               country = ifelse(grepl("melbourne", user_location), "Australia", country),
                               country = ifelse(grepl("sydney", user_location), "Australia", country),
                               country = ifelse(grepl("canada", user_location), "Canada", country),
                               country = ifelse(grepl("africa", user_location), "Africa", country),
                               country = ifelse(grepl("england", user_location), "UK", country),
                               country = ifelse(grepl("united kingdom", user_location), "UK", country),
                               country = ifelse(grepl("london", user_location), "UK", country),
                               country = ifelse(grepl("uk", user_location), "UK", country),
                               country = ifelse(grepl("united states", user_location), "US", country),
                               country = ifelse(grepl("usa", user_location), "US", country),
                               country = ifelse(grepl("us", user_location), "US", country),
                               country = ifelse(grepl("washington", user_location), "US", country),
                               country = ifelse(grepl("new york", user_location), "US", country),
                               country = ifelse(grepl("angeles", user_location), "US", country),
                               country = ifelse(grepl("atlanta", user_location), "US", country),
                               country = ifelse(grepl("california", user_location), "US", country),
                               country = ifelse(grepl("chicago", user_location), "US", country),
                               country = ifelse(grepl("boston", user_location), "US", country),
                               country = ifelse(grepl("philadelphia", user_location), "US", country),
                               country = ifelse(grepl("diego", user_location), "US", country),
                               country = ifelse(grepl("seattle", user_location), "US", country),
                               country = ifelse(grepl("texas", user_location), "US", country),
                               country = ifelse(grepl("nyc", user_location), "US", country),
                               country = ifelse(grepl("vegas", user_location), "US", country),
                               country = ifelse(grepl("francisco", user_location), "US", country),
                               country = ifelse(grepl("florida", user_location), "US", country),
                               country = ifelse(grepl("dallas", user_location), "US", country),
                               country = ifelse(grepl("denver", user_location), "US", country),
                               country = ifelse(grepl("worldwide", user_location), "NoCountry", country),
                               country = ifelse(grepl("global", user_location), "NoCountry", country),
                               country = ifelse(grepl("earth", user_location), "NoCountry", country),
                               country = ifelse(grepl("everywhere", user_location), "NoCountry", country),
                               country = ifelse(grepl("nigeria", user_location), "Nigeria", country),
                               country = ifelse(grepl("kenya", user_location), "Kenya", country),
                               country = ifelse(grepl("switzerland", user_location), "Switzerland", country),
                               country = ifelse(grepl("ireland", user_location), "Ireland", country),
                               country = ifelse(grepl("canada", user_location), "Canada", country),
                               country = ifelse(grepl("toronto", user_location), "Canada", country),
                               country = ifelse(grepl("philippines", user_location), "Philippines", country),
                               country = ifelse(grepl("malaysia", user_location), "Malaysia", country),)
```

```{r}
# Import some additional data
worldcities <- read_csv("~/desktop/Code Implementation//worldcitiespop.csv",
                        col_types = cols(Country = col_character(),
                                         City = col_character(),
                                         AccentCity = col_character(),
                                         Region = col_character(),
                                         Population = col_double(),
                                         Latitude = col_double(),
                                         Longitude = col_double()))
```

```{r}
# US Cities
us_cities <- worldcities %>%
                filter(Country == "us") %>%
                mutate(Country = "US") %>%
                select(Country, City, AccentCity)

# Cross locations with cities to extract the country
tweets_location$flag_us <- purrr::map_df(tweets_location, ~ .x %in% us_cities$City)$user_location
```

```{r}
# Add the new `country` column
tweets <- tweets %>%
            left_join(tweets_location, by = "user_location") %>%
            select(-c(n, flag_us))
```

```{r}
library(dplyr)
#install.packages("tidytext")
library(tidytext)
# Breaks the tweet into words on each row
# in order to append the "sentiment" of the tweet
unnest_tweets <- tweets %>% 
    mutate(text = as.character(tweets$text)) %>% 
    unnest_tokens(word, text)
```

```{r}
#install.packages("stopwords")
library(stopwords)
# Create a dataframe with stopwords
stopwords_script <- tibble(word = c(stopwords("en"), c("thats","weve","hes","theres","ive","im",
                                                           "will","can","cant","dont","youve","us",
                                                           "youre","youll","theyre","whats","didnt", "just")))
```

```{r}
library(lubridate) 
tweets <- tweets %>%
            mutate(day_of_month = mday(date),
                   month = month(date),
                   season = ifelse(month %in% c(12, 1, 2), "Winter",
                                   ifelse(month %in% c(3, 4, 5), "Spring", 
                                          ifelse(month %in% c(6, 7, 8), "Summer", "Winter"))),
                   )
```

```{r}
#install.packages("wordcloud")
library(wordcloud)
#install.packages("reshape2")
library(reshape2)

options(repr.plot.width=15, repr.plot.height=15)

unnest_tweets %>% 
    inner_join(bing, by="word") %>%
    count(word, sentiment, sort=T) %>% 
    acast(word ~ sentiment, value.var = "n", fill=0) %>% 
  
    # wordcloud
    comparison.cloud(colors=my_colors[c(5, 1)], max.words = 400, title.size = 2,
                  scale = c(3,.5))
```

```{r}
options(repr.plot.width=15, repr.plot.height=9)

# The plot:
unnest_tweets %>% 
    inner_join(nrc, "word") %>%
    filter(!sentiment %in% c("positive", "negative")) %>% 
    count(sentiment, sort=T) %>% 

    ggplot(aes(x=reorder(sentiment, n), y=n)) +
    geom_bar(stat="identity", aes(fill=n), show.legend=F) +
    geom_label(aes(label=format(n, big.mark = ",")), size=5, fill="white") +
    labs(x="Sentiment", y="Frequency", title="What is the overall mood in Tweets?") +
    scale_fill_gradient(low = my_colors[3], high = my_colors[1], guide="none") +
    coord_flip() + 
    my_theme + theme(axis.text.x = element_blank())
```

```{r}
options(repr.plot.width=15, repr.plot.height=9)

unnest_tweets %>% 
  inner_join(nrc, "word") %>% 
  count(sentiment, word, sort=T) %>%
  group_by(sentiment) %>% 
  arrange(desc(n)) %>% 
  slice(1:7) %>% 
  
  # Plot:
  ggplot(aes(x=reorder(word, n), y=n)) +
  geom_col(aes(fill=sentiment), show.legend = F) +
  facet_wrap(~sentiment, scales = "free_y", nrow = 2, ncol = 5) +
  coord_flip() +
  my_theme + theme(axis.text.x = element_blank()) +
  labs(x="Word", y="Frequency", title="Sentiment split by most frequent words") +
  scale_fill_manual(values = c(my_colors, "#BE82AF", "#9D4387", "#DEC0D7",
                                 "#40BDC8", "#80D3DB", "#BFE9ED"))

```
##Note: There is quite an even distribution, with most of the words in the positive side, however many words in the negative side as well.
##Extremely positive words: outstanding, thrilled, superb, breathtaking, amazing
##Extremely negative words: some very bad words and swearing that I won't emphasize here (never knew these lexicons catch such vocabulary)
```{r}
options(repr.plot.width=15, repr.plot.height=9)

unnest_tweets %>% 
  # Count how many word per value
  inner_join(afinn, "word") %>% 
  group_by(value) %>% 
  count(value, sort=T)  %>% 
  
  # Plot
  ggplot(aes(x=value, y=n)) +
  geom_bar(stat="identity", show.legend = F, width = 0.5, fill = my_colors[1]) +
  geom_label(aes(label=format(n, big.mark = ",")), size=5) +
  scale_x_continuous(breaks=seq(-5, 5, 1)) +
  labs(x="Score", y="Frequency", title="Word count distribution over intensity of sentiment: Neg - Pos") +
  my_theme + theme(axis.text.y = element_blank())
```

```{r}
options(repr.plot.width=15, repr.plot.height=9)

unnest_tweets %>% 
  # by word and value count number of occurences
  inner_join(afinn, "word") %>% 
  count(word, value, sort=T) %>% 
  mutate(contribution = n * value,
         sentiment = ifelse(contribution<=0, "Negative", "Positive")) %>% #another variable
  arrange(desc(abs(contribution))) %>% 
  head(20)  %>% 
  
  # plot
  ggplot(aes(x=reorder(word, contribution), y=contribution, fill=sentiment)) +
  geom_col(aes(fill=sentiment), show.legend = F) +
  labs(x="Word", y="Contribution", title="Words with biggest contributions in positive/negative sentiments") +
  coord_flip() +
  scale_fill_manual(values=my_colors[c(3, 2)]) + 
  my_theme
```

```{r}
bigram <- frequentBigrams(tweets %>% 
                            filter(country %in% c("US", "UK", "India", "NoCountry")) %>% 
                            select(text) %>% 
                            drop_na(text)) %>%
                head(25)
```
```{r}
options(repr.plot.width=15, repr.plot.height=12)

# Bigram plot
bigram %>%
  ggplot(aes(x=reorder(word, freq), y=freq)) +
  geom_bar(stat="identity", aes(fill=freq), show.legend = F) +
  geom_label(aes(label=freq), size=5) +
  labs(title="Bigram: Most used set of 2 words") +
  scale_fill_gradient(low = my_colors[3], high = my_colors[1], guide="none") +
  my_theme + theme(axis.text.x = element_blank(), axis.title = element_blank()) +
  coord_flip()
```

```{r}
trigram <- frequentTrigrams(tweets %>% 
                            filter(country %in% c("US", "UK", "India", "NoCountry")) %>% 
                            select(text) %>% 
                            drop_na(text)) %>%
                head(25)
```

```{r}
options(repr.plot.width=15, repr.plot.height=12)

# Trigram plot
trigram %>%
  ggplot(aes(x=reorder(word, freq), y=freq)) +
  geom_bar(stat="identity", aes(fill=freq), show.legend = F) +
  geom_label(aes(label=freq), size=5) +
  labs(title="Trigram: Most used set of 3 words") +
  scale_fill_gradient(low = my_colors[3], high = my_colors[1], guide="none") +
  my_theme + theme(axis.text.x = element_blank(), axis.title = element_blank()) +
  coord_flip()
```

```{r}
options(repr.plot.width=15, repr.plot.height=9)

data %>%
    select(Date, Confirmed, Recovered, Deaths) %>%
    gather(key = group_var, value = "Cases", -Date, na.rm = TRUE) %>%
    group_by(Date, group_var) %>%
    summarise(n = sum(Cases, na.rm = F), .groups = "drop_last") %>%
    mutate(label = if_else(Date == max(Date), as.character(group_var), NA_character_)) %>% 

    ggplot(aes(x=Date, y = n, color=group_var)) + 
    geom_line(size = 1.5) +
#     geom_label_repel(aes(label = label), nudge_x = 2, hjust=0, na.rm = TRUE, label.size = 0.1, size=3, segment.size = 0.1)
    scale_color_manual(values = my_colors) +
    scale_linetype_manual(values=c("solid", "twodash", "dotted")) +
    coord_cartesian(clip = 'off') +
    scale_y_continuous(labels = scales::comma) +
    scale_x_date(date_breaks = "months" , date_labels = "%b-%y") +
    my_theme + theme(axis.title.x = element_blank()) +
    labs(title = "Reported Cases in Time", subtitle = "2020", y = "Frequency")
```

```{r}
options(repr.plot.width=15, repr.plot.height=9)

data %>%
    select(Country, Confirmed, Recovered, Deaths) %>%
    gather(key = group_var, value = "Cases", -Country, na.rm = TRUE) %>%
    group_by(Country, group_var) %>%
    summarise(n = sum(Cases, na.rm = F), .groups = "drop_last") %>%
    arrange(desc(n)) %>% 
    group_by(group_var) %>% 
    slice(1:5) %>%

    ggplot(aes(x = Country, y = n, fill=Country)) +
    geom_bar(stat = "identity") +
    facet_grid(~ group_var, scales = "free") +
    scale_fill_manual(values = c(my_colors, "#BE82AF", "#9D4387", "#DEC0D7"), guide="none") +
    geom_label(aes(label=round(n/1000000, 1)), size=5, fill="white") +
    labs(title = "Top Countries per Case Type", subtitle = "Numbers in Millions") +
    my_theme + theme(axis.text.y = element_blank(),
                     axis.text.x = element_text(angle = 30, vjust = 1, hjust = 1),
                     axis.title = element_blank())
```

```{r}
options(repr.plot.width=15, repr.plot.height=9)

data %>%
    filter(State != c("NA", "Unknown")) %>%
    select(State, Confirmed, Recovered, Deaths) %>%
    gather(key = group_var, value = "Cases", -State, na.rm = TRUE) %>%
    group_by(State, group_var) %>%
    summarise(n = sum(Cases, na.rm = F), .groups = "drop_last") %>%
    arrange(desc(n)) %>%
    group_by(group_var) %>% 
    slice(1:5) %>%

    ggplot(aes(x = State, y = n, fill=State)) +
    geom_bar(stat = "identity") +
    facet_grid(~ group_var, scales = "free") +
    scale_fill_manual(values = c(my_colors, "#BE82AF", "#9D4387", "#DEC0D7",
                                 "#40BDC8", "#80D3DB", "#BFE9ED"), guide="none") +
    geom_label(aes(label=round(n/1000000, 1)), size=5, fill="white") +
    labs(title = "Top States per Case Type", subtitle = "Numbers in Millions") +
    my_theme + theme(axis.text.y = element_blank(),
                     axis.text.x = element_text(angle = 30, vjust = 1, hjust = 1),
                     axis.title = element_blank())
```

```{r}
options(repr.plot.width=15, repr.plot.height=9)

tweets %>% 
    select(date) %>% 
    mutate(date = ymd_hms(date)) %>% 
    group_by(date) %>% 
    summarize(n = n(), .groups = "drop_last") %>%

    ggplot(aes(x=date, y = n)) + 
    geom_line(size = 1.5, color = my_colors[1]) +
    coord_cartesian(clip = 'off') +
    my_theme + theme(axis.title.x = element_blank()) +
    labs(title = "Number of Tweets in Time", subtitle = "2020", y = "Frequency")
```

```{r}
options(repr.plot.width=15, repr.plot.height=10)

tweets %>%
    group_by(country) %>%
    summarise(n = n(), .groups = "drop_last") %>%
    filter(country != "NA") %>%

    ggplot(aes(x = reorder(country, n), y = n, fill=n)) +
    geom_bar(stat = "identity") +
    coord_flip() +
    scale_fill_gradient(low=my_colors[2], high=my_colors[6], guide="none") +
    geom_label(aes(label=n), size=5, fill="white") +
    labs(title = "Countries Location for Tweets", subtitle = "--work in progress--") +
    my_theme + theme(axis.text.x = element_blank(),
                     axis.title = element_blank())
```

```{r}
options(repr.plot.width=15, repr.plot.height=9)
labels <- c("user_favourites" = "No. Favourites", "user_followers" = "No. Followers", 
            "user_friends" = "No. Friends")

tweets %>%
    select(user_followers, user_favourites, user_friends) %>%
    gather(key = group_var, value = "Cases", na.rm = TRUE) %>%
    
    ggplot(aes(x = Cases)) +
    geom_boxplot(aes(fill = group_var), outlier.fill = "grey35", outlier.shape = 18, 
                 outlier.alpha = 0.1, outlier.size = 2) +
    facet_grid(~ group_var, scales = "free", labeller = as_labeller(labels)) +
    scale_x_continuous(labels = comma) +
    scale_fill_manual(values = my_colors, guide = "none") +
    labs(title = "User Profile", subtitle = "Profile Size") +
    my_theme + theme(axis.text.y = element_blank(),
                     axis.text.x = element_text(angle = 30, vjust = 1, hjust = 1),
                     axis.title = element_blank())
```


